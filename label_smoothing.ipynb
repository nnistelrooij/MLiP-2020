{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss with Label Smoothing\n",
    "\n",
    "This notebook implements cross entropy loss with label smoothing. The label smoothing loss is tested with dummy input (but in the same format that is expected for Bengali.AI) and tested against cross entropy without label smoothing to confirm that the results are the same when smoothing is set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropySumLoss(nn.Module):\n",
    "    \"\"\"Neural network module to compute sum of cross entropy losses.\n",
    "\n",
    "    Attributes:\n",
    "        device = [torch.device] device to compute the loss on\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device):\n",
    "        \"\"\"Initializes the loss module\n",
    "\n",
    "        Args:\n",
    "            device = [torch.device] device to compute the loss on\n",
    "        \"\"\"\n",
    "        super(CrossEntropySumLoss, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"Sums cross entropy losses of given predictions and targets.\n",
    "\n",
    "        Args:\n",
    "            input  = [tuple] sequence of tensors of (raw) predictions\n",
    "            target = [tuple] sequence of tensors of targets\n",
    "\n",
    "        Returns [torch.Tensor]:\n",
    "            The grapheme_root, vowel_dacritic, consonant_diacritic,\n",
    "            and combined losses given the predictions and targets.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for y, t in zip(input, target):\n",
    "#             t = t.to(self.device)\n",
    "            loss = F.cross_entropy(y, t)\n",
    "            losses.append(loss)\n",
    "\n",
    "        losses.append(sum(losses))\n",
    "        return torch.stack(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from:\n",
    "    https://github.com/pytorch/pytorch/issues/7455#issuecomment-513062631\n",
    "    \n",
    "    Cross entropy loss with label smoothing.    \n",
    "    When `smoothing=0.0`, the loss will be equivalent to \n",
    "    standard cross entropy loss (`F.cross_entropy`).\n",
    "    \"\"\"\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            classes   = [tuple] number of classes for grapheme_root, \n",
    "                        vowel_diacritic, and consonant_diacritic\n",
    "            smoothing = [float] controls degree of smoothing\n",
    "            dim       = [int] dimension to compute the loss over\n",
    "        \"\"\"\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing # alpha\n",
    "        self.classes =  classes # (graph, vowel, consonant) \n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred   = [tuple] sequence of tensors of (raw) predictions\n",
    "            target = [tuple] sequence of tensors of targets\n",
    "            \n",
    "        Returns [torch.Tensor]:\n",
    "            The grapheme_root, vowel_dacritic, consonant_diacritic,\n",
    "            and combined losses given the predictions and targets.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for y, t, cls in zip(pred, target, self.classes):\n",
    "            y = y.log_softmax(dim=self.dim)  \n",
    "            with torch.no_grad():\n",
    "                true_dist = torch.zeros_like(y)\n",
    "                true_dist.fill_(self.smoothing / (cls - 1))\n",
    "                true_dist.scatter_(1, t.data.unsqueeze(1), self.confidence)\n",
    "                losses.append( torch.mean(torch.sum(-true_dist * y, dim=self.dim)) )\n",
    "        losses.append(sum(losses))\n",
    "        return torch.stack(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize label smoothing loss\n",
    "classes = (3, 2, 4)\n",
    "loss = LabelSmoothingLoss(classes, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0312, 1.9518, 0.2873, 3.2703])\n"
     ]
    }
   ],
   "source": [
    "# Dummy input\n",
    "g = torch.tensor( [ [0.2, 1.6, 0.5], [8.5, 0.3, 3.2] ] )\n",
    "v = torch.tensor( [ [2.1, 0.2, 0.1, 0.8], [1.5, 0.4, 4.2, 0.2] ] )\n",
    "c = torch.tensor( [ [0.5, 3.2], [2.1, 1.1] ] )\n",
    "\n",
    "gt = torch.tensor( [1, 0] )\n",
    "vt = torch.tensor( [0, 2] )\n",
    "ct = torch.tensor( [1, 0] )\n",
    "\n",
    "y = (g, v, c)\n",
    "t = (gt, vt, ct)\n",
    "lsl = loss.forward(y, t)\n",
    "print(lsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2312, 0.2727, 0.1892, 0.6931])\n"
     ]
    }
   ],
   "source": [
    "ce = CrossEntropySumLoss(None)\n",
    "cel = ce.forward(y, t)\n",
    "print(cel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when smoothing=0, label smoothing and cross entropy loss should be equal\n",
    "assert torch.all( torch.eq(lsl, cel) ), 'Not equal.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
