{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwXxGn5FkAGE"
   },
   "source": [
    "# Bengali.AI - Training and Evaluation\n",
    "\n",
    "The notebook was created in Kaggle. It contains the training and evaluation (with a validation set) pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BpTgaePiAvtH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9s1Ps_HAvtO"
   },
   "source": [
    "I changed `torch.utils.data._utils.collate` to change `torch.stack()` in to `torch.cat()` at line 24 below. The behaviour of the `DataLoader` was made simpler and I needed to do it to prevent errors due to different tensor shapes for each tuple that was retrieved from `BengaliDataset.__getitem__()`. So please do not be alarmed by this enormously complex code in the below cell; I did not write it and do not understand it myself. I just needed to change it to get the preferred `DataLoader` behaviour, i.e. no errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DB9SjDSpAvtQ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from torch._six import container_abcs, string_classes, int_classes\n",
    "\n",
    "np_str_obj_array_pattern = re.compile(r'[SaUO]')\n",
    "default_collate_err_msg_format = (\n",
    "    \"default_collate: batch must contain tensors, numpy arrays, numbers, \"\n",
    "    \"dicts or lists; found {}\")\n",
    "\n",
    "def _new_default_collate(batch):\n",
    "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
    "\n",
    "    elem = batch[0]\n",
    "    elem_type = type(elem)\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        out = None\n",
    "        if torch.utils.data.get_worker_info() is not None:\n",
    "            # If we're in a background process, concatenate directly into a\n",
    "            # shared memory tensor to avoid an extra copy\n",
    "            numel = sum([x.numel() for x in batch])\n",
    "            storage = elem.storage()._new_shared(numel)\n",
    "            out = elem.new(storage)\n",
    "        return torch.cat(batch, 0, out=out)\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        elem = batch[0]\n",
    "        if elem_type.__name__ == 'ndarray':\n",
    "            # array of string classes and object\n",
    "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
    "                raise TypeError(default_collate_err_msg_format.format(elem.dtype))\n",
    "\n",
    "            return _new_default_collate([torch.as_tensor(b) for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int_classes):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, string_classes):\n",
    "        return batch\n",
    "    elif isinstance(elem, container_abcs.Mapping):\n",
    "        return {key: _new_default_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n",
    "        return elem_type(*(_new_default_collate(samples) for samples in zip(*batch)))\n",
    "    elif isinstance(elem, container_abcs.Sequence):\n",
    "        transposed = zip(*batch)\n",
    "        return [_new_default_collate(samples) for samples in transposed]\n",
    "\n",
    "    raise TypeError(default_collate_err_msg_format.format(elem_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MhtWUpchkAGS"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary  # git clone https://github.com/sksq96/pytorch-summary\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2AzEzcjoTek"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200840, 128, 128)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = np.load('train_image_data.npy')\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "9KUkq0SMkAGY",
    "outputId": "6d051c47-13aa-431d-fde8-3fa28b58967b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   grapheme_root  vowel_diacritic  consonant_diacritic\n",
       "0             15                9                    5\n",
       "1            159                0                    0\n",
       "2             22                3                    5\n",
       "3             53                2                    2\n",
       "4             71                9                    5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = pd.read_csv('train.csv').iloc[:, 1:-1]\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train-validation split 80/20\n",
    "train_images, val_images = train_test_split(train_images,\n",
    "                                            test_size=0.20,\n",
    "                                            random_state=2020)\n",
    "train_labels, val_labels = train_test_split(train_labels,\n",
    "                                            test_size=0.20,\n",
    "                                            random_state=2020)\n",
    "gc.collect() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroNet(nn.Module):\n",
    "    def __init__(self, stride=1, kernel_size=3, padding=1, bias=False):\n",
    "        super(ZeroNet, self).__init__()\n",
    "\n",
    "        # images are 128 * 128\n",
    "        # conv channels based on practice from MNIST networks\n",
    "        # input channels 1, output channels 10\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)\n",
    "        \n",
    "        # input channels 10, output channels 20, \n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        # extra fully-connected layers to determine labels\n",
    "        # 128 * 128 * 20/2 = 3380\n",
    "        self.fc0 = nn.Linear(128, 3380)\n",
    "        self.fc1 = nn.Linear(3380, 256)\n",
    "        self.fc2 = nn.Linear(256, 168)\n",
    "        self.fc3 = nn.Linear(256, 11)\n",
    "        self.fc4 = nn.Linear(256, 7)\n",
    "        \n",
    "    def _split_vectors(self, vectors, num_augments):\n",
    "        \"\"\"Splits the latent vectors into tensors for each subproblem.\n",
    "        \n",
    "        Splits the latent vectors according to the number of augmentations per\n",
    "        image for each subproblem. It returns three tensors that contain a \n",
    "        subset of the latent vectors in vecs to increase efficiency.\n",
    "        \n",
    "        Args:\n",
    "            vectors      = [torch.Tensor] the latent vectors to be split\n",
    "            num_augments = [torch.Tensor] number of augmentations per sub-\n",
    "                                          problem with shape (BATCH_SIZE, 3)\n",
    "                                          \n",
    "        Returns [torch.Tensor]*3:\n",
    "            The latent vectors for the grapheme_root, vowel_diacritic,\n",
    "            and consonant_diacritic subproblems.\n",
    "        \"\"\"\n",
    "        if not num_augments:\n",
    "            return vectors, vectors, vectors\n",
    "        \n",
    "        # determine the slices of the latent vectors for each subproblem\n",
    "        max_augments, _ = num_augments.max(dim=1, keepdim=True)\n",
    "        diffs = torch.cat((torch.zeros(1, 1).long(), max_augments))                           \n",
    "        start_indices = torch.cumsum(diffs, dim=0)[:-1]\n",
    "        slices = torch.cat((start_indices, start_indices + num_augments), dim=1)\n",
    "        \n",
    "        # determine the indices of the latent vectors for each subproblem\n",
    "        graph = torch.cat([torch.arange(st,end) for st,end in slices[:, [0,1]]])\n",
    "        vowel = torch.cat([torch.arange(st,end) for st,end in slices[:, [0,2]]])\n",
    "        conso = torch.cat([torch.arange(st,end) for st,end in slices[:, [0,3]]])\n",
    "\n",
    "        return vectors[graph], vectors[vowel], vectors[conso]\n",
    "\n",
    "    def forward(self, x, num_augments=None):\n",
    "        \"\"\"Foward pass of the CNN.\n",
    "        \n",
    "        Args:\n",
    "            x            = [torch.Tensor] images with shape (N, 1, SIZE, SIZE)\n",
    "            num_augments = [torch.Tensor] number of augmentations per sub-\n",
    "                                          problem with shape (BATCH_SIZE, 3)\n",
    "        \n",
    "        Returns [torch.Tensor]*3:\n",
    "            Non-normalized predictions for each class for each subproblem.\n",
    "        \"\"\"\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 3))\n",
    "        x = x.view(len(x), -1)  # flatten representation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # x = self.fc2(x)\n",
    "        \n",
    "        x_graph, x_vowel, x_conso = self._split_vectors(x, num_augments)\n",
    "        y_graph = self.fc2(x_graph)\n",
    "        y_vowel = self.fc3(x_vowel)\n",
    "        y_conso = self.fc4(x_conso)\n",
    "        # return F.log_softmax(x)\n",
    "        return y_graph, y_vowel, y_conso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0XlmbNckAGq"
   },
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    \"\"\"Class to augment images with cutout: https://arxiv.org/abs/1708.04552.\n",
    "    \n",
    "    Attributes:\n",
    "        num_squares = [int] number of squares to cut out of the image\n",
    "        length      = [int] the length (in pixels) of each square\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_squares, length):    \n",
    "        \"\"\"Initialize cutout augmentation.\n",
    "        \n",
    "        Args:\n",
    "            num_squares = [int] number of squares to cut out of the image\n",
    "            length      = [int] the length (in pixels) of each square\n",
    "        \"\"\"\n",
    "        self.num_squares = num_squares\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \"\"\"Randomly mask out one or more squares from an image.\n",
    "\n",
    "        Args:\n",
    "            image = [torch.Tensor] image of shape (1, SIZE, SIZE)\n",
    "\n",
    "        Returns [torch.Tensor]:\n",
    "            Image with num_squares of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        # determine center of squares\n",
    "        coords = torch.randint(high=SIZE, size=(2, self.num_squares))\n",
    "\n",
    "        # determine top-left and bottom-right corners of squares\n",
    "        x1, y1 = torch.clamp(coords - self.length // 2, 0, SIZE)\n",
    "        x2, y2 = torch.clamp(coords + self.length // 2, 0, SIZE)\n",
    "\n",
    "        # cut squares out of image\n",
    "        for x1, y1, x2, y2 in zip(x1, y1, x2, y2):\n",
    "            image[:, y1:y2, x1:x2] = 0\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "class BengaliDataset(Dataset):\n",
    "    \"\"\"Class to get images and labels.\n",
    "    \n",
    "    Attributes:\n",
    "        images         = [ndarray] images array with shape (N, SIZE, SIZE)\n",
    "        transform      = [Compose] applies a random affine transformation,\n",
    "                                   normalizes to z-scores, and applies cutout\n",
    "                                   transformation to a Numpy array image\n",
    "        normalize      = [Normalize] normalizes Numpy array image to z-scores\n",
    "        labels         = [torch.Tensor] images labels tensor of shape (N, 3)\n",
    "        mod_counts     = [torch.Tensor] remainders of dividing each class\n",
    "                                        frequency by the highest frequency\n",
    "        ratio_counts   = [torch.Tensor] floors of dividing each class\n",
    "                                        frequency by the highest frequency\n",
    "        current_counts = [torch.Tensor] number of retrieved items of each\n",
    "                                        class in current iteration of epoch\n",
    "        device         = [torch.device] device to put images and labels on\n",
    "        augment        = [bool] whether or not the images are transformed\n",
    "        balance        = [bool] whether or not the classes are balanced\n",
    "    \"\"\"\n",
    "    def __init__(self, images, labels, device, augment=False, balance=False):\n",
    "        \"\"\"Initialize dataset.\n",
    "        \n",
    "        Args:\n",
    "            images  = [ndarray] images array with shape (N, SIZE, SIZE)\n",
    "            labels  = [DataFrame] image labels DataFrame of shape (N, 3)\n",
    "            device  = [torch.device] device to put images and labels on\n",
    "            augment = [bool] whether or not the images are transformed\n",
    "            balance = [bool] whether or not the classes are balanced            \n",
    "        \"\"\"\n",
    "        super(Dataset, self).__init__()\n",
    "        \n",
    "        # initialize transformations from torchvision.transforms\n",
    "        self.images = images\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomAffine(\n",
    "                degrees=(-8, 8),\n",
    "                translate=(1/24, 1/24),\n",
    "                scale=(8/9, 10/9)\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.071371482,), std=(0.20764154,)),\n",
    "            Cutout(8, 12)\n",
    "        ])\n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.071371482,), std=(0.20764154,))\n",
    "        ])\n",
    "        \n",
    "        # initialize labels and counts for class balancing\n",
    "        self.labels = torch.tensor(labels.to_numpy(), device=device)\n",
    "        counts = labels.apply(pd.Series.value_counts).to_numpy().T\n",
    "        max_counts = np.nanmax(counts, axis=1, keepdims=True)\n",
    "        self.mod_counts = torch.tensor(max_counts % counts)\n",
    "        self.ratio_counts = torch.tensor(max_counts // counts)\n",
    "        self.current_counts = torch.zeros_like(self.mod_counts)\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.augment = augment\n",
    "        self.balance = balance\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset number of retrieved items of each class in current epoch.\"\"\"\n",
    "        self.current_counts = torch.zeros_like(self.mod_counts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def _num_augmentations(self, labels):\n",
    "        \"\"\"Computes number of augmentations for given image labels.\n",
    "        \n",
    "        Args:\n",
    "            labels = [torch.Tensor] image labels of shape (3)\n",
    "            \n",
    "        Returns [torch.Tensor]:\n",
    "            If self.balance is False, a tensor filled with ones is returned.\n",
    "            Otherwise, the number of augmentations will ensure that all the\n",
    "            classes are seen the same number of times for each subproblem.\n",
    "        \"\"\"\n",
    "        if not self.balance:  # one augmentation\n",
    "            return torch.tensor([1]*len(labels))\n",
    "        \n",
    "        # select current and modular counts for given labels\n",
    "        current_counts = self.current_counts[[0, 1, 2], labels]\n",
    "        self.current_counts[[0, 1, 2], labels] += 1\n",
    "        mod_counts = self.mod_counts[[0, 1, 2], labels]\n",
    "\n",
    "        # determine number of augmentations with possible extra augmentation\n",
    "        extra_augment = current_counts < mod_counts\n",
    "        num_augments = self.ratio_counts[[0, 1, 2], labels] + extra_augment\n",
    "\n",
    "        return num_augments.long()\n",
    "\n",
    "    def _augment_or_normalize(self, image):\n",
    "        \"\"\"Augments (including normalization) or normalizes image.\n",
    "        \n",
    "        Args:\n",
    "            image = [ndarray] Numpy array image of shape (SIZE, SIZE)\n",
    "            \n",
    "        Returns [torch.Tensor]\n",
    "            Augmented or normalized image with shape (1, 1, SIZE, SIZE).\n",
    "        \"\"\"\n",
    "        if self.augment:  # random affine, normalize, cutout\n",
    "            image = self.transform(image)\n",
    "        else:  # normalize\n",
    "            image = self.normalize(image)\n",
    "\n",
    "        return image.to(self.device).unsqueeze(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get images, labels, and number of augmentations.\n",
    "        \n",
    "        Args:\n",
    "            idx = [int] index of original image and labels\n",
    "            \n",
    "        Returns [torch.Tensor]*5:\n",
    "            images       = images tensor of shape (N, 1, SIZE, SIZE)\n",
    "            labels_graph = labels tensor of grapheme_root subproblem\n",
    "            labels_vowel = labels tensor of vowel_diacritic subproblem\n",
    "            labels_conso = labels tensor of consonant_diacritic subproblem\n",
    "            num_augments = number of augmentations of shape (1, 3)\n",
    "        \"\"\"\n",
    "        # select image and labels\n",
    "        image = self.images[idx]\n",
    "        labels = self.labels[idx]\n",
    "        \n",
    "        # determine number of augmentations per subproblem\n",
    "        num_augments = self._num_augmentations(labels)\n",
    "        \n",
    "        # transform or normalize image\n",
    "        images = self._augment_or_normalize(image)\n",
    "        for _ in range(max(num_augments) - 1):\n",
    "            images = torch.cat((images, self._augment_or_normalize(image)))\n",
    "\n",
    "        # repeat labels given number of augmentations\n",
    "        labels = [labels[i].repeat(num_augments[i]) for i in range(len(labels))]\n",
    "\n",
    "        # return images, labels, and number of augmentations as a 5-tuple\n",
    "        return (images,) + tuple(labels) + (num_augments.unsqueeze(0),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4Yk4QRlkAG7"
   },
   "outputs": [],
   "source": [
    "def eval_metric(pred_dict, true_dict):\n",
    "    \"\"\"\n",
    "    Competition evaluation metric adapted from:\n",
    "    https://www.kaggle.com/c/bengaliai-cv19/overview/evaluation\n",
    "    The metric describes the weighted average of component \n",
    "    macro-averaged recalls.\n",
    "    \n",
    "    Args:\n",
    "        pred_dict = [dict] dictionary with components as keys and\n",
    "                           lists of predictions as values\n",
    "        true_dict = [dict] dictionary with components as key and\n",
    "                           lists of targets as values\n",
    "    \n",
    "    Returns [float]:\n",
    "        Weighted average of component macro-averaged recalls.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for key in ['grapheme', 'vowel', 'consonant']:\n",
    "        score = recall_score(true_dict[key], pred_dict[key], average='macro')\n",
    "        scores.append(score)\n",
    "    return np.average(scores, weights=[2, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3lZbC2IckAHA"
   },
   "outputs": [],
   "source": [
    "def update_dicts(pred_dict, true_dict, preds, targets):\n",
    "    \"\"\"Updates two dictionaries given batches of values.\n",
    "    \n",
    "    Args:\n",
    "        pred_dict = [dict] dictionary with components as keys and\n",
    "                           lists of predictions as values\n",
    "        true_dict = [dict] dictionary with components as key and\n",
    "                           lists of targets as values\n",
    "        preds     = [tuple] sequence of tensors of (raw) predictions\n",
    "        targets   = [tuple] sequence of tensors of targets\n",
    "    \"\"\"\n",
    "    for key, y, t in zip(['grapheme', 'vowel', 'consonant'], preds, targets):\n",
    "        _, pred = torch.max(y.data, 1)\n",
    "        pred_list = pred.tolist()\n",
    "        target_list = t.tolist()\n",
    "        pred_dict[key] += pred_list\n",
    "        true_dict[key] += target_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pEzITCBkAHI"
   },
   "source": [
    "Naming convention:\n",
    "- `x` = input\n",
    "- `t` = target\n",
    "- `y` = predicted output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbjosgSlAvuK"
   },
   "outputs": [],
   "source": [
    "def criterion(preds, targets):\n",
    "    \"\"\"Sums cross entropy losses of given predictions and targets.\n",
    "    \n",
    "    Args:\n",
    "        preds   = [tuple] sequence of tensors of (raw) predictions\n",
    "        targets = [tuple] sequence of tensors of targets\n",
    "    \n",
    "    Returns [torch.Tensor]:\n",
    "        Combined loss of given predictions and targets.\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for y, t in zip(preds, targets):\n",
    "        loss += F.cross_entropy(y, t)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader):\n",
    "    \"\"\"Computes loss and score of current state of model on validation dataset.\n",
    "    \n",
    "    Args:\n",
    "        model       = [nn.Module] model to test with validation dataset\n",
    "        val_loader  = [DataLoader] validation data loader\n",
    "        \n",
    "    Returns [float]*2:\n",
    "        loss  = average loss per batch on the validation dataset\n",
    "        score = weighted average of component macro-averaged recalls\n",
    "    \"\"\"\n",
    "    # set model mode to evaluation\n",
    "    model = model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    pred_dict = {'grapheme': [], 'vowel': [], 'consonant': []}\n",
    "    true_dict = {'grapheme': [], 'vowel': [], 'consonant': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            x, t_graph, t_vowel, t_conso, _ = data\n",
    "            \n",
    "            # predict\n",
    "            y = model(x)\n",
    "            \n",
    "            # loss\n",
    "            t = t_graph, t_vowel, t_conso\n",
    "            loss = criterion(y, t)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # update pred_dict and true_dict\n",
    "            update_dicts(pred_dict, true_dict, y, t)\n",
    "    \n",
    "    # set model mode back to training\n",
    "    model = model.train()\n",
    "            \n",
    "    return running_loss/len(val_loader), eval_metric(pred_dict, true_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qW6ECh0vkAHL"
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataset, train_loader, val_loader, optimizer,\n",
    "          epochs=10, path=PATH):\n",
    "    \"\"\"Trains the model given train data and validates it given validation data.\n",
    "    \n",
    "    Args:\n",
    "        model         = [nn.Module] model to train and validate\n",
    "        train_dataset = [Dataset] train dataset\n",
    "        train_loader  = [DataLoader] train data loader\n",
    "        val_loader    = [DataLoader] validation data loader\n",
    "        optimizer     = [Optimizer] optimizer to update the model\n",
    "        epochs        = [int] number of iterations of the train dataset\n",
    "        path          = [str] file path where the model weights are saved\n",
    "        \n",
    "    Returns [[float]]*4:\n",
    "        train_losses = list of average train loss per batch for each epoch\n",
    "        train_scores = list of train Kaggle scores for each epoch\n",
    "        val_losses = list of average validation loss per batch for each epoch\n",
    "        val_scores = list of validation Kaggle scores for each epoch\n",
    "    \"\"\"\n",
    "    train_losses = [] # keep track of losses on training set\n",
    "    train_scores = [] # keep track of scores on training set\n",
    "    val_losses = [] # keep track of losses on validation set\n",
    "    val_scores = [] # keep track of scores on validation set\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        pred_dict = {'grapheme': [], 'vowel': [], 'consonant': []}\n",
    "        true_dict = {'grapheme': [], 'vowel': [], 'consonant': []}\n",
    "        \n",
    "        for data in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
    "            x, t_graph, t_vowel, t_conso, num_augments = data # depends on what is returned by Dataset.__getitem__\n",
    "\n",
    "            # predict\n",
    "            y = model(x, num_augments)\n",
    "\n",
    "            # loss\n",
    "            t = t_graph, t_vowel, t_conso\n",
    "            loss = criterion(y, t)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # update\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # update pred_dict and true_dict\n",
    "            update_dicts(pred_dict, true_dict, y, t)\n",
    "            \n",
    "        train_score = eval_metric(pred_dict, true_dict)\n",
    "        val_loss, val_score = validation(model, val_loader) # validation loss and score\n",
    "        \n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        train_scores.append(train_score)\n",
    "        val_losses.append(val_loss)\n",
    "        val_scores.append(val_score)\n",
    "        \n",
    "        print('Train loss: {:.3f}'.format( running_loss / len(train_loader) ),\n",
    "              '\\tVal loss: {:.3f}'.format( val_loss ))\n",
    "        print('Train score: {:.3f}'.format( train_score ), \n",
    "              '\\tVal score: {:.3f}'.format( val_score ))\n",
    "        \n",
    "        # reset dataset to keep class balance\n",
    "        train_dataset.reset()\n",
    "        \n",
    "    # save model weights to path\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "    return train_losses, train_scores, val_losses, val_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niels\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:94: RuntimeWarning: invalid value encountered in remainder\n",
      "C:\\Users\\Niels\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: RuntimeWarning: invalid value encountered in floor_divide\n"
     ]
    }
   ],
   "source": [
    "SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "PATH = 'model.pt'\n",
    "\n",
    "# use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# initialize network and show summary\n",
    "model = ZeroNet().to(device).train()\n",
    "input_size = [(1, SIZE, SIZE), (3,)]  # [(channels, H, W), (num_subproblems,)]\n",
    "dtypes = (torch.float32, torch.int64)  # (images, num_augments)\n",
    "summary(model, input_size, device=device, dtypes=dtypes)\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training set\n",
    "train_dataset = BengaliDataset(train_images, train_labels, device,\n",
    "                               augment=False, balance=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, collate_fn=_new_default_collate)\n",
    "\n",
    "# validation set\n",
    "val_dataset = BengaliDataset(val_images, val_labels, device,\n",
    "                             augment=False, balance=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, collate_fn=_new_default_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335,
     "referenced_widgets": [
      "6186b6f0e04c456a86905920b4466326",
      "9fea6664ee7246e3b6c262ecdc10b76f",
      "1fc6df9388aa48619e3ed1c5ad82aa00",
      "aad8becc556d4085baa1edf443a0c7a2",
      "0fc405aa2ebb44068cdd539563d9241c",
      "fb2e2ca679e14b8895044b99045e0772",
      "222a4673913947c2b510bc115a7b450e",
      "9281404443994c60b287c7d3dea976f7"
     ]
    },
    "colab_type": "code",
    "id": "Nei3Dc8YkAHR",
    "outputId": "8037b43d-02cf-4598-e841-960882d4f47f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34328cb5aa364109b3a5a1b909b5e2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=79.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.045985\n",
      "0:00:00.037988\n",
      "0:00:00.009996\n",
      "0:00:00.036970\n",
      "0:00:00.035970\n",
      "0:00:00.041986\n",
      "0:00:00.050966\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 4.00 GiB total capacity; 1.28 GiB already allocated; 534.79 MiB free; 970.00 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-82a8b0502f0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m losses_scores = train(model, train_dataset, train_loader, val_dataset,\n\u001b[1;32m----> 2\u001b[1;33m                       val_loader, optimizer, epochs=5)\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-7e68eacb917e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataset, train_loader, val_dataset, val_loader, optimizer, epochs, path)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;31m# update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 4.00 GiB total capacity; 1.28 GiB already allocated; 534.79 MiB free; 970.00 MiB cached)"
     ]
    }
   ],
   "source": [
    "losses_scores = train(model, train_dataset, train_loader, val_loader,\n",
    "                      optimizer, epochs=5)\n",
    "train_losses, train_scores, val_losses, val_scores = losses_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TWtYCmNXkAHW"
   },
   "outputs": [],
   "source": [
    "def plot_perf(train_losses, train_scores, val_losses, val_scores):\n",
    "    epochs = len(train_losses)\n",
    "    x_range = range(1, epochs + 1)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "    ax[0].plot(x_range, train_losses, label='train')\n",
    "    ax[0].plot(x_range, val_losses, label='validation')\n",
    "    ax[0].set_xticks(x_range)\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_title('Average loss over epochs', size=14)\n",
    "    ax[0].legend()\n",
    "    ax[1].plot(x_range, train_scores, label='train')\n",
    "    ax[1].plot(x_range, val_scores, label='validation')\n",
    "    ax[1].set_xticks(x_range)\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Score')\n",
    "    ax[1].set_title('Score over epochs', size=14)\n",
    "    ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WRPPKCPgkAHa",
    "outputId": "58e8260b-d73b-4e14-b155-efdc6b17b32b"
   },
   "outputs": [],
   "source": [
    "plot_perf(train_losses, train_scores, val_losses, val_scores)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "GCP Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fc405aa2ebb44068cdd539563d9241c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "1fc6df9388aa48619e3ed1c5ad82aa00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "Epoch 1",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb2e2ca679e14b8895044b99045e0772",
      "max": 6277,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fc405aa2ebb44068cdd539563d9241c",
      "value": 5
     }
    },
    "222a4673913947c2b510bc115a7b450e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6186b6f0e04c456a86905920b4466326": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1fc6df9388aa48619e3ed1c5ad82aa00",
       "IPY_MODEL_aad8becc556d4085baa1edf443a0c7a2"
      ],
      "layout": "IPY_MODEL_9fea6664ee7246e3b6c262ecdc10b76f"
     }
    },
    "9281404443994c60b287c7d3dea976f7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fea6664ee7246e3b6c262ecdc10b76f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aad8becc556d4085baa1edf443a0c7a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9281404443994c60b287c7d3dea976f7",
      "placeholder": "​",
      "style": "IPY_MODEL_222a4673913947c2b510bc115a7b450e",
      "value": "  0% 5/6277 [00:17&lt;5:58:42,  3.43s/it]"
     }
    },
    "fb2e2ca679e14b8895044b99045e0772": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
