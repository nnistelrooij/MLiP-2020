{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCP Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "503c4d93b7294d2fa3a1597f84fcfa53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5dfd0bad4827437eb332242e1622c64d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_405fcdf509544cd389965ebb5a0469b5",
              "IPY_MODEL_0df6b53b555241d2a0527aafeeb3ff19"
            ]
          }
        },
        "5dfd0bad4827437eb332242e1622c64d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "405fcdf509544cd389965ebb5a0469b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_90bb80b82f754e6a9c86b2c41447b0b7",
            "_dom_classes": [],
            "description": "Epoch 1/50",
            "_model_name": "IntProgressModel",
            "bar_style": "danger",
            "max": 1256,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 95,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1ff78128d3b43d9ade8466cdf8c64ff"
          }
        },
        "0df6b53b555241d2a0527aafeeb3ff19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0a28d56726f44115abeb2a73c3492446",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  8% 95/1256 [00:29&lt;05:52,  3.29it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ddf0afd21d8a480789789b7485333cce"
          }
        },
        "90bb80b82f754e6a9c86b2c41447b0b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1ff78128d3b43d9ade8466cdf8c64ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a28d56726f44115abeb2a73c3492446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ddf0afd21d8a480789789b7485333cce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cwXxGn5FkAGE"
      },
      "source": [
        "# Bengali.AI - Training and Evaluation\n",
        "\n",
        "The notebook was created in Kaggle. It contains the training and evaluation (with a validation set) pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BpTgaePiAvtH",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N9s1Ps_HAvtO"
      },
      "source": [
        "I changed `torch.utils.data._utils.collate` to change `torch.stack()` in to `torch.cat()` at line 24 below. The behaviour of the `DataLoader` was made simpler and I needed to do it to prevent errors due to different tensor shapes for each tuple that was retrieved from `BengaliDataset.__getitem__()`. So please do not be alarmed by this enormously complex code in the below cell; I did not write it and do not understand it myself. I just needed to change it to get the preferred `DataLoader` behaviour, i.e. no errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DB9SjDSpAvtQ",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "from torch._six import container_abcs, string_classes, int_classes\n",
        "\n",
        "np_str_obj_array_pattern = re.compile(r'[SaUO]')\n",
        "default_collate_err_msg_format = (\n",
        "    \"default_collate: batch must contain tensors, numpy arrays, numbers, \"\n",
        "    \"dicts or lists; found {}\")\n",
        "\n",
        "def _new_default_collate(batch):\n",
        "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
        "\n",
        "    elem = batch[0]\n",
        "    elem_type = type(elem)\n",
        "    if isinstance(elem, torch.Tensor):\n",
        "        out = None\n",
        "        if torch.utils.data.get_worker_info() is not None:\n",
        "            # If we're in a background process, concatenate directly into a\n",
        "            # shared memory tensor to avoid an extra copy\n",
        "            numel = sum([x.numel() for x in batch])\n",
        "            storage = elem.storage()._new_shared(numel)\n",
        "            out = elem.new(storage)\n",
        "        return torch.cat(batch, 0, out=out)\n",
        "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
        "            and elem_type.__name__ != 'string_':\n",
        "        elem = batch[0]\n",
        "        if elem_type.__name__ == 'ndarray':\n",
        "            # array of string classes and object\n",
        "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
        "                raise TypeError(default_collate_err_msg_format.format(elem.dtype))\n",
        "\n",
        "            return _new_default_collate([torch.as_tensor(b) for b in batch])\n",
        "        elif elem.shape == ():  # scalars\n",
        "            return torch.as_tensor(batch)\n",
        "    elif isinstance(elem, float):\n",
        "        return torch.tensor(batch, dtype=torch.float64)\n",
        "    elif isinstance(elem, int_classes):\n",
        "        return torch.tensor(batch)\n",
        "    elif isinstance(elem, string_classes):\n",
        "        return batch\n",
        "    elif isinstance(elem, container_abcs.Mapping):\n",
        "        return {key: _new_default_collate([d[key] for d in batch]) for key in elem}\n",
        "    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n",
        "        return elem_type(*(_new_default_collate(samples) for samples in zip(*batch)))\n",
        "    elif isinstance(elem, container_abcs.Sequence):\n",
        "        transposed = zip(*batch)\n",
        "        return [_new_default_collate(samples) for samples in transposed]\n",
        "\n",
        "    raise TypeError(default_collate_err_msg_format.format(elem_type))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MhtWUpchkAGS",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary  # pip install torchsummary\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v2AzEzcjoTek",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aef41646-9743-4827-f45a-1e3f01cb9afa"
      },
      "source": [
        "train_images = np.load('train_image_data.npy')\n",
        "train_images.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200840, 128, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab_type": "code",
        "id": "9KUkq0SMkAGY",
        "outputId": "e37cab39-f5e5-4918-a6d0-71cfd0eef84a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "train_labels = pd.read_csv('train.csv').iloc[:, 1:-1]\n",
        "train_labels.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>grapheme_root</th>\n",
              "      <th>vowel_diacritic</th>\n",
              "      <th>consonant_diacritic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>159</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>71</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   grapheme_root  vowel_diacritic  consonant_diacritic\n",
              "0             15                9                    5\n",
              "1            159                0                    0\n",
              "2             22                3                    5\n",
              "3             53                2                    2\n",
              "4             71                9                    5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_fN596GlX-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1f163ad-c299-4dcf-f428-2524a549890f"
      },
      "source": [
        "# train-validation split 80/20\n",
        "train_images, val_images = train_test_split(train_images, test_size=0.20,\n",
        "                                            random_state=2020)\n",
        "train_labels, val_labels = train_test_split(train_labels, test_size=0.20,\n",
        "                                            random_state=2020)\n",
        "gc.collect() # garbage collection"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CT7xXPRlX-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ZeroNet(nn.Module):\n",
        "  \n",
        "    def __init__(self, device, kernel_size=3):\n",
        "        super(ZeroNet, self).__init__()\n",
        "\n",
        "        # images are 128 * 128\n",
        "        # conv channels based on practice from MNIST networks\n",
        "        # input channels 1, output channels 10\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=kernel_size)\n",
        "        \n",
        "        # input channels 10, output channels 20, \n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=kernel_size)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "\n",
        "        # extra fully-connected layers to determine labels\n",
        "        # 128 * 128 * 20/2 = 3380\n",
        "        self.fc1 = nn.Linear(3380, 256)\n",
        "        self.fc2 = nn.Linear(256, 168)\n",
        "        self.fc3 = nn.Linear(256, 11)\n",
        "        self.fc4 = nn.Linear(256, 7)\n",
        "\n",
        "        # put model on GPU\n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "        \n",
        "    def _split_vectors(self, vectors, num_augments):\n",
        "        \"\"\"Splits the latent vectors into tensors for each subproblem.\n",
        "        \n",
        "        Splits the latent vectors according to the number of augmentations per\n",
        "        image for each subproblem. It returns three tensors that contain a \n",
        "        subset of the latent vectors in vecs to increase efficiency.\n",
        "        \n",
        "        Args:\n",
        "            vectors      = [torch.Tensor] the latent vectors to be split\n",
        "            num_augments = [torch.Tensor] number of augmentations per sub-\n",
        "                                          problem with shape (BATCH_SIZE, 3)\n",
        "                                          \n",
        "        Returns [torch.Tensor]*3:\n",
        "            The latent vectors for the grapheme_root, vowel_diacritic,\n",
        "            and consonant_diacritic subproblems.\n",
        "        \"\"\"\n",
        "        if num_augments is None:\n",
        "            return vectors, vectors, vectors\n",
        "        \n",
        "        # determine the slices of the latent vectors for each subproblem\n",
        "        max_augments, _ = num_augments.max(dim=1, keepdim=True)\n",
        "        diffs = torch.cat((torch.zeros(1, 1).long(), max_augments))                           \n",
        "        start_indices = torch.cumsum(diffs, dim=0)[:-1]\n",
        "        slices = torch.cat((start_indices, start_indices + num_augments), dim=1)\n",
        "        \n",
        "        # determine the indices of the latent vectors for each subproblem\n",
        "        graph = torch.cat([torch.arange(st,end) for st,end in slices[:, [0,1]]])\n",
        "        vowel = torch.cat([torch.arange(st,end) for st,end in slices[:, [0,2]]])\n",
        "        conso = torch.cat([torch.arange(st,end) for st,end in slices[:, [0,3]]])\n",
        "\n",
        "        return vectors[graph], vectors[vowel], vectors[conso]\n",
        "\n",
        "    def forward(self, x, num_augments=None):\n",
        "        \"\"\"Foward pass of the CNN.\n",
        "        \n",
        "        Args:\n",
        "            x            = [torch.Tensor] images with shape (N, 1, SIZE, SIZE)\n",
        "            num_augments = [torch.Tensor] number of augmentations per sub-\n",
        "                                          problem with shape (BATCH_SIZE, 3)\n",
        "        \n",
        "        Returns [torch.Tensor]*3:\n",
        "            Non-normalized predictions for each class for each subproblem.\n",
        "        \"\"\"\n",
        "        # put images on GPU\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 3))\n",
        "        x = x.view(len(x), -1)  # flatten representation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # x = F.dropout(x, training=self.training)\n",
        "        # x = self.fc2(x)\n",
        "        \n",
        "        x_graph, x_vowel, x_conso = self._split_vectors(x, num_augments)\n",
        "        y_graph = self.fc2(x_graph)\n",
        "        y_vowel = self.fc3(x_vowel)\n",
        "        y_conso = self.fc4(x_conso)\n",
        "        return y_graph, y_vowel, y_conso"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C0XlmbNckAGq",
        "colab": {}
      },
      "source": [
        "class Cutout(object):\n",
        "    \"\"\"Class to augment images with cutout: https://arxiv.org/abs/1708.04552.\n",
        "    \n",
        "    Attributes:\n",
        "        num_squares = [int] number of squares to cut out of the image\n",
        "        length      = [int] the length (in pixels) of each square\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_squares, length):    \n",
        "        \"\"\"Initialize cutout augmentation.\n",
        "        \n",
        "        Args:\n",
        "            num_squares = [int] number of squares to cut out of the image\n",
        "            length      = [int] the length (in pixels) of each square\n",
        "        \"\"\"\n",
        "        self.num_squares = num_squares\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, image):\n",
        "        \"\"\"Randomly mask out one or more squares from an image.\n",
        "\n",
        "        Args:\n",
        "            image = [torch.Tensor] image of shape (1, SIZE, SIZE)\n",
        "\n",
        "        Returns [torch.Tensor]:\n",
        "            Image with num_squares of dimension length x length cut out of it.\n",
        "        \"\"\"\n",
        "        # determine center of squares\n",
        "        coords = torch.randint(high=SIZE, size=(2, self.num_squares))\n",
        "\n",
        "        # determine top-left and bottom-right corners of squares\n",
        "        x1, y1 = torch.clamp(coords - self.length // 2, 0, SIZE)\n",
        "        x2, y2 = torch.clamp(coords + self.length // 2, 0, SIZE)\n",
        "\n",
        "        # cut squares out of image\n",
        "        for x1, y1, x2, y2 in zip(x1, y1, x2, y2):\n",
        "            image[:, y1:y2, x1:x2] = 0\n",
        "\n",
        "        return image\n",
        "    \n",
        "    \n",
        "class BengaliDataset(Dataset):\n",
        "    \"\"\"Class to get images and labels.\n",
        "    \n",
        "    Attributes:\n",
        "        images         = [ndarray] images array with shape (N, SIZE, SIZE)\n",
        "        transform      = [Compose] applies a random affine transformation,\n",
        "                                   normalizes to z-scores, and applies cutout\n",
        "                                   transformation to a Numpy array image\n",
        "        normalize      = [Normalize] normalizes Numpy array image to z-scores\n",
        "        labels         = [torch.Tensor] images labels tensor of shape (N, 3)\n",
        "        mod_counts     = [torch.Tensor] remainders of dividing each class\n",
        "                                        frequency by the highest frequency\n",
        "        ratio_counts   = [torch.Tensor] floors of dividing each class\n",
        "                                        frequency by the highest frequency\n",
        "        current_counts = [torch.Tensor] number of retrieved items of each\n",
        "                                        class in current iteration of epoch\n",
        "        augment        = [bool] whether or not the images are transformed\n",
        "        balance        = [bool] whether or not the classes are balanced\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, images, labels, augment=False, balance=False):\n",
        "        \"\"\"Initialize dataset.\n",
        "        \n",
        "        Args:\n",
        "            images  = [ndarray] images array with shape (N, SIZE, SIZE)\n",
        "            labels  = [DataFrame] image labels DataFrame of shape (N, 3)\n",
        "            augment = [bool] whether or not the images are transformed\n",
        "            balance = [bool] whether or not the classes are balanced            \n",
        "        \"\"\"\n",
        "        super(Dataset, self).__init__()\n",
        "        \n",
        "        # initialize transformations from torchvision.transforms\n",
        "        self.images = images\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomAffine(\n",
        "                degrees=(-8, 8),\n",
        "                translate=(1/24, 1/24),\n",
        "                scale=(8/9, 10/9)\n",
        "            ),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.071371482,), std=(0.20764154,)),\n",
        "            Cutout(8, 12)\n",
        "        ])\n",
        "        self.normalize = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.071371482,), std=(0.20764154,))\n",
        "        ])\n",
        "        \n",
        "        # initialize labels and counts for class balancing\n",
        "        self.labels = torch.tensor(labels.to_numpy())\n",
        "        counts = labels.apply(pd.Series.value_counts).to_numpy().T\n",
        "        max_counts = np.nanmax(counts, axis=1, keepdims=True)\n",
        "        self.mod_counts = torch.tensor(max_counts % counts)\n",
        "        self.ratio_counts = torch.tensor(max_counts // counts)\n",
        "        self.current_counts = torch.zeros_like(self.mod_counts)\n",
        "        \n",
        "        self.augment = augment\n",
        "        self.balance = balance\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reset number of retrieved items of each class in current epoch.\"\"\"\n",
        "        self.current_counts = torch.zeros_like(self.mod_counts)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def _num_augmentations(self, labels):\n",
        "        \"\"\"Computes number of augmentations for given image labels.\n",
        "        \n",
        "        Args:\n",
        "            labels = [torch.Tensor] image labels of shape (3,)\n",
        "            \n",
        "        Returns [torch.Tensor]:\n",
        "            If self.balance is False, a tensor filled with ones is returned.\n",
        "            Otherwise, the number of augmentations will ensure that all the\n",
        "            classes are seen the same number of times for each subproblem.\n",
        "        \"\"\"\n",
        "        if not self.balance:  # one augmentation\n",
        "            return torch.tensor([1]*len(labels))\n",
        "        \n",
        "        # select current and modular counts for given labels\n",
        "        current_counts = self.current_counts[[0, 1, 2], labels]\n",
        "        self.current_counts[[0, 1, 2], labels] += 1\n",
        "        mod_counts = self.mod_counts[[0, 1, 2], labels]\n",
        "\n",
        "        # determine number of augmentations with possible extra augmentation\n",
        "        extra_augment = current_counts < mod_counts\n",
        "        num_augments = self.ratio_counts[[0, 1, 2], labels] + extra_augment\n",
        "\n",
        "        return num_augments.long()\n",
        "\n",
        "    def _augment_or_normalize(self, image):\n",
        "        \"\"\"Augments (including normalization) or normalizes image.\n",
        "        \n",
        "        Args:\n",
        "            image = [ndarray] Numpy array image of shape (SIZE, SIZE)\n",
        "            \n",
        "        Returns [torch.Tensor]\n",
        "            Augmented or normalized image with shape (1, 1, SIZE, SIZE).\n",
        "        \"\"\"\n",
        "        if self.augment:  # random affine, normalize, cutout\n",
        "            image = self.transform(image)\n",
        "        else:  # normalize\n",
        "            image = self.normalize(image)\n",
        "\n",
        "        return image.unsqueeze(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get images, labels, and number of augmentations.\n",
        "        \n",
        "        Args:\n",
        "            idx = [int] index of original image and labels\n",
        "            \n",
        "        Returns [torch.Tensor]*5:\n",
        "            images       = images tensor of shape (N, 1, SIZE, SIZE)\n",
        "            labels_graph = labels tensor of grapheme_root subproblem\n",
        "            labels_vowel = labels tensor of vowel_diacritic subproblem\n",
        "            labels_conso = labels tensor of consonant_diacritic subproblem\n",
        "            num_augments = number of augmentations of shape (1, 3)\n",
        "        \"\"\"\n",
        "        # select image and labels\n",
        "        image = self.images[idx]\n",
        "        labels = self.labels[idx]\n",
        "        \n",
        "        # determine number of augmentations per subproblem\n",
        "        num_augments = self._num_augmentations(labels)\n",
        "        \n",
        "        # transform or normalize image\n",
        "        images = self._augment_or_normalize(image)\n",
        "        for _ in range(max(num_augments) - 1):\n",
        "            images = torch.cat((images, self._augment_or_normalize(image)))\n",
        "\n",
        "        # repeat labels given number of augmentations\n",
        "        labels = [labels[i].repeat(num_augments[i]) for i in range(len(labels))]\n",
        "\n",
        "        # return images, labels, and number of augmentations as a 5-tuple\n",
        "        return (images,) + tuple(labels) + (num_augments.unsqueeze(0),)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y4Yk4QRlkAG7",
        "colab": {}
      },
      "source": [
        "def eval_metric(pred_dict, true_dict):\n",
        "    \"\"\"\n",
        "    Competition evaluation metric adapted from:\n",
        "    https://www.kaggle.com/c/bengaliai-cv19/overview/evaluation\n",
        "    The metric describes the weighted average of component \n",
        "    macro-averaged recalls.\n",
        "    \n",
        "    Args:\n",
        "        pred_dict = [dict] dictionary with components as keys and\n",
        "                           lists of predictions as values\n",
        "        true_dict = [dict] dictionary with components as key and\n",
        "                           lists of targets as values\n",
        "    \n",
        "    Returns [float]*4:\n",
        "        grapheme  = grapheme_root component macro-average recall\n",
        "        vowel     = vowel_diacritic component macro-average recall\n",
        "        consonant = consonant_diacritic component macro-average recall\n",
        "        total     = weighted average of component macro-averaged recalls\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    for key in ['grapheme', 'vowel', 'consonant']:\n",
        "        score = recall_score(true_dict[key], pred_dict[key], average='macro')\n",
        "        scores.append(score)\n",
        "        \n",
        "    scores.append(np.average(scores, weights=[2, 1, 1]))\n",
        "    return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3lZbC2IckAHA",
        "colab": {}
      },
      "source": [
        "def update_dicts(pred_dict, true_dict, preds, targets):\n",
        "    \"\"\"Updates two dictionaries given batches of values.\n",
        "    \n",
        "    Args:\n",
        "        pred_dict = [dict] dictionary with components as keys and\n",
        "                           lists of predictions as values\n",
        "        true_dict = [dict] dictionary with components as key and\n",
        "                           lists of targets as values\n",
        "        preds     = [tuple] sequence of tensors of (raw) predictions\n",
        "        targets   = [tuple] sequence of tensors of targets\n",
        "    \"\"\"\n",
        "    for key, y, t in zip(['grapheme', 'vowel', 'consonant'], preds, targets):\n",
        "        _, pred = torch.max(y.data, 1)\n",
        "        pred_list = pred.tolist()\n",
        "        target_list = t.tolist()\n",
        "        pred_dict[key] += pred_list\n",
        "        true_dict[key] += target_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pEzITCBkAHI"
      },
      "source": [
        "Naming convention:\n",
        "- `x` = input\n",
        "- `t` = target\n",
        "- `y` = predicted output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LbjosgSlAvuK",
        "colab": {}
      },
      "source": [
        "class CrossEntropySumLoss(nn.Module):\n",
        "    \"\"\"Neural network module to compute sum of cross entropy losses.\n",
        "\n",
        "    Attributes:\n",
        "        device = [torch.device] device to compute the loss on\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device):\n",
        "        \"\"\"Initializes the loss module\n",
        "\n",
        "        Args:\n",
        "            device = [torch.device] device to compute the loss on\n",
        "        \"\"\"\n",
        "        super(CrossEntropySumLoss, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"Sums cross entropy losses of given predictions and targets.\n",
        "        \n",
        "        Args:\n",
        "            input  = [tuple] sequence of tensors of (raw) predictions\n",
        "            target = [tuple] sequence of tensors of targets\n",
        "        \n",
        "        Returns [torch.Tensor]:\n",
        "            The grapheme_root, vowel_dacritic, consonant_diacritic,\n",
        "            and combined losses given the predictions and targets.\n",
        "        \"\"\"\n",
        "        losses = torch.zeros(1, dtype=torch.float32, device=self.device)\n",
        "        for y, t in zip(input, target):\n",
        "            t = t.to(self.device)\n",
        "            loss = F.cross_entropy(y, t).view(1)\n",
        "            losses = torch.cat((loss, losses))\n",
        "\n",
        "        losses[-1] = sum(losses[:-1])        \n",
        "        return losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--IKV3-5lX-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_iterations = 0\n",
        "num_batches = 0\n",
        "running_losses = torch.zeros(4)\n",
        "pred_dict = {'grapheme': [], 'vowel': [], 'consonant': []}\n",
        "true_dict = {'grapheme': [], 'vowel': [], 'consonant': []}\n",
        "\n",
        "def show_metrics(writer, losses=None, preds=None, targets=None,\n",
        "                 inc=True, eval_freq=100, end=False):\n",
        "    \"\"\"Show the losses and scores on TensorBoard.\n",
        "    \n",
        "    Args:\n",
        "        writer    = [SummaryWriter] TensorBoard writer of metrics\n",
        "        losses    = [torch.Tensor] subproblem losses and combined loss\n",
        "        preds     = [tuple] sequence of tensors of (raw) predictions\n",
        "        targets   = [tuple] sequence of tensors of targets\n",
        "        inc       = [bool] whether to increment the number of iterations\n",
        "        eval_freq = [int] number of iterations before the next TensorBoard\n",
        "                          update; if set to -1, TensorBoard never updates\n",
        "        end       = [bool] always shows metrics after epoch has ended\n",
        "    \"\"\"\n",
        "    global num_iterations\n",
        "    global num_batches\n",
        "    global running_losses\n",
        "    global pred_dict\n",
        "    global true_dict   \n",
        "    \n",
        "    if not end:\n",
        "        # increment total number of training iterations during run\n",
        "        num_iterations += inc\n",
        "\n",
        "        # increment number of batches during current epoch\n",
        "        num_batches += 1\n",
        "        \n",
        "        # accumulate metrics to smooth plots\n",
        "        running_losses += losses.data.cpu()\n",
        "        update_dicts(pred_dict, true_dict, preds, targets)\n",
        "    \n",
        "    # show metrics every eval_freq iterations or at the end of an epoch\n",
        "    if num_iterations % eval_freq == (eval_freq - 1) or end:\n",
        "        # show losses in TensorBoard\n",
        "        losses = running_losses / num_batches\n",
        "        writer.add_scalar('Loss/grapheme_root',\n",
        "                          losses[0], num_iterations)\n",
        "        writer.add_scalar('Loss/vowel_diacritic',\n",
        "                          losses[1], num_iterations)\n",
        "        writer.add_scalar('Loss/consonant_diacritic',\n",
        "                          losses[2], num_iterations)\n",
        "        writer.add_scalar('Loss/total',\n",
        "                          losses[3], num_iterations)\n",
        "\n",
        "        # show scores in TensorBoard\n",
        "        scores = eval_metric(pred_dict, true_dict)\n",
        "        writer.add_scalar('Score/grapheme_root',\n",
        "                          scores[0], num_iterations)\n",
        "        writer.add_scalar('Score/vowel_diacritic',\n",
        "                          scores[1], num_iterations)\n",
        "        writer.add_scalar('Score/consonant_diacritic',\n",
        "                          scores[2], num_iterations)\n",
        "        writer.add_scalar('Score/total',\n",
        "                          scores[3], num_iterations)\n",
        "        \n",
        "        # reset running variables\n",
        "        num_batches = 0\n",
        "        running_losses = torch.zeros(4)\n",
        "        pred_dict = {'grapheme': [], 'vowel': [], 'consonant': []}\n",
        "        true_dict = {'grapheme': [], 'vowel': [], 'consonant': []}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKueMB5RlX-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(model, val_loader, val_writer, criterion):\n",
        "    \"\"\"Computes loss and score of current state of model on validation dataset.\n",
        "    \n",
        "    Args:\n",
        "        model      = [nn.Module] model to test with validation dataset\n",
        "        val_loader = [DataLoader] validation data loader\n",
        "        val_writer = [SummaryWriter] TensorBoard writer of validation metrics\n",
        "        criterion  = [nn.Module] neural network module to compute loss\n",
        "    \"\"\"\n",
        "    # set model mode to evaluation\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            x, t_graph, t_vowel, t_conso, _ = data\n",
        "            \n",
        "            # predict\n",
        "            y = model(x)\n",
        "            \n",
        "            # loss\n",
        "            t = t_graph, t_vowel, t_conso\n",
        "            losses = criterion(y, t)\n",
        "            \n",
        "            # accumulate but do not show validation metrics\n",
        "            show_metrics(val_writer, losses, y, t, inc=False, eval_freq=-1)\n",
        "            \n",
        "    # show validation metrics on TensorBoard\n",
        "    show_metrics(val_writer, end=True)\n",
        "    \n",
        "    # set model mode back to training\n",
        "    model = model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qW6ECh0vkAHL",
        "colab": {}
      },
      "source": [
        "def train(model, train_dataset, train_loader, train_writer,\n",
        "          val_loader, val_writer, optimizer, criterion, num_epochs=10):\n",
        "    \"\"\"Trains the model given train data and validates it given validation data.\n",
        "    \n",
        "    Args:\n",
        "        model         = [nn.Module] model to train and validate\n",
        "        train_dataset = [Dataset] train dataset\n",
        "        train_loader  = [DataLoader] train data loader\n",
        "        train_writer  = [SummaryWriter] TensorBoard writer of train metrics\n",
        "        val_loader    = [DataLoader] validation data loader\n",
        "        val_writer    = [SummaryWriter] TensorBoard writer of validation metrics\n",
        "        optimizer     = [Optimizer] optimizer to update the model\n",
        "        criterion     = [nn.Module] neural network module to compute loss\n",
        "        num_epochs    = [int] number of iterations of the train dataset\n",
        "    \"\"\"\n",
        "    current_time = datetime.now()\n",
        "    for epoch in range(num_epochs):\n",
        "        for data in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
        "            x, t_graph, t_vowel, t_conso, num_augments = data # depends on what is returned by Dataset.__getitem__                \n",
        "\n",
        "            # predict\n",
        "            y = model(x, num_augments)\n",
        "\n",
        "            # loss\n",
        "            t = t_graph, t_vowel, t_conso\n",
        "            losses = criterion(y, t)\n",
        "\n",
        "            # update\n",
        "            optimizer.zero_grad() \n",
        "            losses[-1].backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # show train metrics every 100 iterations in TensorBoard\n",
        "            show_metrics(train_writer, losses, y, t)\n",
        "\n",
        "        # show train metrics at end of epoch\n",
        "        show_metrics(train_writer, end=True)\n",
        "                \n",
        "        # evaluate model on validation data\n",
        "        validation(model, val_loader, val_writer, criterion)\n",
        "        \n",
        "        # reset dataset to keep class balance\n",
        "        train_dataset.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMlCPo-nlX-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "9397aa19-526a-4d67-ae82-ebc2330d971d"
      },
      "source": [
        "SIZE = 128\n",
        "BATCH_SIZE = 128\n",
        "PATH = 'model.pt'\n",
        "\n",
        "# use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "# initialize network and show summary\n",
        "model = ZeroNet(device).train()\n",
        "summary(model, (1, SIZE, SIZE), device=str(device))  # input_size = (1, SIZE, SIZE)\n",
        "\n",
        "# initialize optimizer and criterion\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = CrossEntropySumLoss(device)\n",
        "\n",
        "# training set\n",
        "train_dataset = BengaliDataset(train_images, train_labels,\n",
        "                               augment=False, balance=False)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=4, collate_fn=_new_default_collate,\n",
        "                          pin_memory=True)\n",
        "\n",
        "# validation set\n",
        "val_dataset = BengaliDataset(val_images, val_labels,\n",
        "                             augment=False, balance=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=4, collate_fn=_new_default_collate,\n",
        "                        pin_memory=True)\n",
        "\n",
        "# TensorBoard writers\n",
        "current_time = datetime.now().strftime(\"%Y-%m-%d/%H'%M'%S\")\n",
        "train_writer = SummaryWriter(f'runs/{current_time}/train')\n",
        "train_writer.add_graph(model, iter(train_loader).next()[0])\n",
        "val_writer = SummaryWriter(f'runs/{current_time}/validation')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 10, 126, 126]             100\n",
            "            Conv2d-2           [-1, 20, 40, 40]           1,820\n",
            "         Dropout2d-3           [-1, 20, 40, 40]               0\n",
            "            Linear-4                  [-1, 256]         865,536\n",
            "            Linear-5                  [-1, 168]          43,176\n",
            "            Linear-6                   [-1, 11]           2,827\n",
            "            Linear-7                    [-1, 7]           1,799\n",
            "================================================================\n",
            "Total params: 915,258\n",
            "Trainable params: 915,258\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 1.70\n",
            "Params size (MB): 3.49\n",
            "Estimated Total Size (MB): 5.26\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:72: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nei3Dc8YkAHR",
        "outputId": "41661fbd-9f77-42e1-f0c8-d2b6cf9f3c91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "503c4d93b7294d2fa3a1597f84fcfa53",
            "5dfd0bad4827437eb332242e1622c64d",
            "405fcdf509544cd389965ebb5a0469b5",
            "0df6b53b555241d2a0527aafeeb3ff19",
            "90bb80b82f754e6a9c86b2c41447b0b7",
            "f1ff78128d3b43d9ade8466cdf8c64ff",
            "0a28d56726f44115abeb2a73c3492446",
            "ddf0afd21d8a480789789b7485333cce"
          ]
        }
      },
      "source": [
        "train(model, train_dataset, train_loader, train_writer,\n",
        "      val_loader, val_writer, optimizer, criterion, num_epochs=50)\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "503c4d93b7294d2fa3a1597f84fcfa53",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch 1/50', max=1256, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-04a6b944014c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train(model, train_dataset, train_loader, train_writer,\n\u001b[0;32m----> 2\u001b[0;31m       val_loader, val_writer, optimizer, criterion, num_epochs=50)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-793d81b0dc5f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, train_loader, train_writer, val_loader, val_writer, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}