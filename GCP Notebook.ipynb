{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCP Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "42ccdb5125fd4cc789827c3aa6b25b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_65c30daa4181472298f6c158db902500",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ce81f3f41ae449289fec4f9dd4b5c3b6",
              "IPY_MODEL_28ea77d195db42dbac6747ef56a6951a"
            ]
          }
        },
        "65c30daa4181472298f6c158db902500": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce81f3f41ae449289fec4f9dd4b5c3b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7f2c9a48e8714157aac1cfce45d4350b",
            "_dom_classes": [],
            "description": "Epoch 1/50",
            "_model_name": "IntProgressModel",
            "bar_style": "danger",
            "max": 1256,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 343,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3915de1396e343eebecb8b5e949787e9"
          }
        },
        "28ea77d195db42dbac6747ef56a6951a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f8eb4a08163045339793547bb8803c64",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 27% 343/1256 [00:29&lt;00:51, 17.63it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c7048b6a73bb4a6e829f146eef1fda55"
          }
        },
        "7f2c9a48e8714157aac1cfce45d4350b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3915de1396e343eebecb8b5e949787e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8eb4a08163045339793547bb8803c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c7048b6a73bb4a6e829f146eef1fda55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cwXxGn5FkAGE"
      },
      "source": [
        "# Bengali.AI - Training and Evaluation\n",
        "\n",
        "The notebook was created in Kaggle. It contains the training and evaluation (with a validation set) pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BpTgaePiAvtH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "e9a6301e-3f11-4b88-ed52-2878b0d3f11e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N9s1Ps_HAvtO"
      },
      "source": [
        "I added communication with TensorBoard via the `SummaryWriter` class. It shows the losses and scores of the three subproblems and the total loss and score for all three subproblems. I also used multiprocessing to load the data. On Windows (and possibly Mac OS) this unfortunately only works on files < 2 GB, where the images are 3.2 GB. But on the GCP VM it does work. On that VM, it could do 20 iterations of 128 images per second. The proportion of time that the model is run and trained is small compared to the time to load the data. This means that the next step is to increase the model complexity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nifNjf76aoWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "import gc\n",
        "import re\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch._six import container_abcs, string_classes, int_classes\n",
        "from torchsummary import summary  # pip install torchsummary\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DB9SjDSpAvtQ",
        "colab": {}
      },
      "source": [
        "np_str_obj_array_pattern = re.compile(r'[SaUO]')\n",
        "default_collate_err_msg_format = (\n",
        "    \"default_collate: batch must contain tensors, numpy arrays, numbers, \"\n",
        "    \"dicts or lists; found {}\")\n",
        "\n",
        "\n",
        "def _new_default_collate(batch):\n",
        "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
        "\n",
        "    elem = batch[0]\n",
        "    elem_type = type(elem)\n",
        "    if isinstance(elem, torch.Tensor):\n",
        "        out = None\n",
        "        if torch.utils.data.get_worker_info() is not None:\n",
        "            # If we're in a background process, concatenate directly into a\n",
        "            # shared memory tensor to avoid an extra copy\n",
        "            numel = sum([x.numel() for x in batch])\n",
        "            storage = elem.storage()._new_shared(numel)\n",
        "            out = elem.new(storage)\n",
        "        return torch.cat(batch, 0, out=out)\n",
        "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
        "            and elem_type.__name__ != 'string_':\n",
        "        elem = batch[0]\n",
        "        if elem_type.__name__ == 'ndarray':\n",
        "            # array of string classes and object\n",
        "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
        "                raise TypeError(default_collate_err_msg_format.format(elem.dtype))\n",
        "\n",
        "            return _new_default_collate([torch.as_tensor(b) for b in batch])\n",
        "        elif elem.shape == ():  # scalars\n",
        "            return torch.as_tensor(batch)\n",
        "    elif isinstance(elem, float):\n",
        "        return torch.tensor(batch, dtype=torch.float64)\n",
        "    elif isinstance(elem, int_classes):\n",
        "        return torch.tensor(batch)\n",
        "    elif isinstance(elem, string_classes):\n",
        "        return batch\n",
        "    elif isinstance(elem, container_abcs.Mapping):\n",
        "        return {key: _new_default_collate([d[key] for d in batch]) for key in elem}\n",
        "    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n",
        "        return elem_type(*(_new_default_collate(samples) for samples in zip(*batch)))\n",
        "    elif isinstance(elem, container_abcs.Sequence):\n",
        "        transposed = zip(*batch)\n",
        "        return [_new_default_collate(samples) for samples in transposed]\n",
        "\n",
        "    raise TypeError(default_collate_err_msg_format.format(elem_type))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v2AzEzcjoTek",
        "outputId": "47253c74-aaaa-4eab-f6d4-367de24eee9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_images = np.load('drive/My Drive/train_image_data.npy')\n",
        "train_images.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200840, 128, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab_type": "code",
        "id": "9KUkq0SMkAGY",
        "outputId": "d04fb0bf-86fe-4a25-a812-1a32a863f463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "train_labels = pd.read_csv('drive/My Drive/train.csv').iloc[:, 1:-1]\n",
        "train_labels.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>grapheme_root</th>\n",
              "      <th>vowel_diacritic</th>\n",
              "      <th>consonant_diacritic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>159</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>71</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   grapheme_root  vowel_diacritic  consonant_diacritic\n",
              "0             15                9                    5\n",
              "1            159                0                    0\n",
              "2             22                3                    5\n",
              "3             53                2                    2\n",
              "4             71                9                    5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2_fN596GlX-E",
        "outputId": "4e44563b-b01b-46ef-ff30-c08eabee779d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# train-validation split 80/20\n",
        "train_images, val_images = train_test_split(train_images, test_size=0.20,\n",
        "                                            random_state=2020)\n",
        "train_labels, val_labels = train_test_split(train_labels, test_size=0.20,\n",
        "                                            random_state=2020)\n",
        "gc.collect()  # garbage collection"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6CT7xXPRlX-L",
        "colab": {}
      },
      "source": [
        "class ZeroNet(nn.Module):\n",
        "  \n",
        "    def __init__(self, device, kernel_size=3):\n",
        "        super(ZeroNet, self).__init__()\n",
        "\n",
        "        # images are 128 * 128\n",
        "        # conv channels based on practice from MNIST networks\n",
        "        # input channels 1, output channels 10\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=kernel_size)\n",
        "        \n",
        "        # input channels 10, output channels 20, \n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=kernel_size)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "\n",
        "        # extra fully-connected layers to determine labels\n",
        "        # 128 * 128 * 20/2 = 3380\n",
        "        self.fc1 = nn.Linear(3380, 256)\n",
        "        self.fc2 = nn.Linear(256, 168)\n",
        "        self.fc3 = nn.Linear(256, 11)\n",
        "        self.fc4 = nn.Linear(256, 7)\n",
        "\n",
        "        # put model on GPU\n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "        \n",
        "    @staticmethod\n",
        "    def _split_vectors(vectors, num_augments):\n",
        "        \"\"\"Splits the latent vectors into tensors for each subproblem.\n",
        "        \n",
        "        Splits the latent vectors according to the number of augmentations per\n",
        "        image for each subproblem. It returns three tensors that contain a \n",
        "        subset of the latent vectors in vecs to increase efficiency.\n",
        "        \n",
        "        Args:\n",
        "            vectors      = [torch.Tensor] the latent vectors to be split\n",
        "            num_augments = [torch.Tensor] number of augmentations per sub-\n",
        "                                          problem with shape (BATCH_SIZE, 3)\n",
        "                                          \n",
        "        Returns [torch.Tensor]*3:\n",
        "            The latent vectors for the grapheme_root, vowel_diacritic,\n",
        "            and consonant_diacritic subproblems.\n",
        "        \"\"\"\n",
        "        if num_augments is None:\n",
        "            return vectors, vectors, vectors\n",
        "        \n",
        "        # determine the slices of the latent vectors for each subproblem\n",
        "        max_augments, _ = num_augments.max(dim=1, keepdim=True)\n",
        "        diffs = torch.cat((torch.zeros(1, 1).long(), max_augments))                           \n",
        "        start_indices = torch.cumsum(diffs, dim=0)[:-1]\n",
        "        slices = torch.cat((start_indices, start_indices + num_augments), dim=1)\n",
        "        \n",
        "        # determine the indices of the latent vectors for each subproblem\n",
        "        graph = torch.cat([torch.arange(st,end) for st,end in slices[:, [0,1]]])\n",
        "        vowel = torch.cat([torch.arange(st,end) for st,end in slices[:, [0,2]]])\n",
        "        conso = torch.cat([torch.arange(st,end) for st,end in slices[:, [0,3]]])\n",
        "\n",
        "        return vectors[graph], vectors[vowel], vectors[conso]\n",
        "\n",
        "    def forward(self, x, num_augments=None):\n",
        "        \"\"\"Foward pass of the CNN.\n",
        "        \n",
        "        Args:\n",
        "            x            = [torch.Tensor] images with shape (N, 1, SIZE, SIZE)\n",
        "            num_augments = [torch.Tensor] number of augmentations per sub-\n",
        "                                          problem with shape (BATCH_SIZE, 3)\n",
        "        \n",
        "        Returns [torch.Tensor]*3:\n",
        "            Non-normalized predictions for each class for each subproblem.\n",
        "        \"\"\"\n",
        "        # put images on GPU\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 3))\n",
        "        x = x.view(len(x), -1)  # flatten representation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # x = F.dropout(x, training=self.training)\n",
        "        # x = self.fc2(x)\n",
        "        \n",
        "        x_graph, x_vowel, x_conso = self._split_vectors(x, num_augments)\n",
        "        y_graph = self.fc2(x_graph)\n",
        "        y_vowel = self.fc3(x_vowel)\n",
        "        y_conso = self.fc4(x_conso)\n",
        "        return y_graph, y_vowel, y_conso"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C0XlmbNckAGq",
        "colab": {}
      },
      "source": [
        "class Cutout(object):\n",
        "    \"\"\"Class to augment images with cutout: https://arxiv.org/abs/1708.04552.\n",
        "    \n",
        "    Attributes:\n",
        "        num_squares = [int] number of squares to cut out of the image\n",
        "        length      = [int] the length (in pixels) of each square\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_squares, length):    \n",
        "        \"\"\"Initialize cutout augmentation.\n",
        "        \n",
        "        Args:\n",
        "            num_squares = [int] number of squares to cut out of the image\n",
        "            length      = [int] the length (in pixels) of each square\n",
        "        \"\"\"\n",
        "        self.num_squares = num_squares\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, image):\n",
        "        \"\"\"Randomly mask out one or more squares from an image.\n",
        "\n",
        "        Args:\n",
        "            image = [torch.Tensor] image of shape (1, SIZE, SIZE)\n",
        "\n",
        "        Returns [torch.Tensor]:\n",
        "            Image with num_squares of dimension length x length cut out of it.\n",
        "        \"\"\"\n",
        "        # determine center of squares\n",
        "        coords = torch.randint(high=SIZE, size=(2, self.num_squares))\n",
        "\n",
        "        # determine top-left and bottom-right corners of squares\n",
        "        x1, y1 = torch.clamp(coords - self.length // 2, 0, SIZE)\n",
        "        x2, y2 = torch.clamp(coords + self.length // 2, 0, SIZE)\n",
        "\n",
        "        # cut squares out of image\n",
        "        for x1, y1, x2, y2 in zip(x1, y1, x2, y2):\n",
        "            image[:, y1:y2, x1:x2] = 0\n",
        "\n",
        "        return image\n",
        "    \n",
        "    \n",
        "class BengaliDataset(Dataset):\n",
        "    \"\"\"Class to get images and labels.\n",
        "    \n",
        "    Attributes:\n",
        "        images         = [ndarray] images array with shape (N, SIZE, SIZE)\n",
        "        transform      = [Compose] applies a random affine transformation,\n",
        "                                   normalizes to z-scores, and applies cutout\n",
        "                                   transformation to a Numpy array image\n",
        "        normalize      = [Normalize] normalizes Numpy array image to z-scores\n",
        "        labels         = [torch.Tensor] images labels tensor of shape (N, 3)\n",
        "        mod_counts     = [torch.Tensor] remainders of dividing each class\n",
        "                                        frequency by the highest frequency\n",
        "        ratio_counts   = [torch.Tensor] floors of dividing each class\n",
        "                                        frequency by the highest frequency\n",
        "        current_counts = [torch.Tensor] number of retrieved items of each\n",
        "                                        class in current iteration of epoch\n",
        "        augment        = [bool] whether or not the images are transformed\n",
        "        balance        = [bool] whether or not the classes are balanced\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, images, labels, augment=False, balance=False):\n",
        "        \"\"\"Initialize dataset.\n",
        "        \n",
        "        Args:\n",
        "            images  = [ndarray] images array with shape (N, SIZE, SIZE)\n",
        "            labels  = [DataFrame] image labels DataFrame of shape (N, 3)\n",
        "            augment = [bool] whether or not the images are transformed\n",
        "            balance = [bool] whether or not the classes are balanced            \n",
        "        \"\"\"\n",
        "        super(Dataset, self).__init__()\n",
        "        \n",
        "        # initialize transformations from torchvision.transforms\n",
        "        self.images = images\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomAffine(\n",
        "                degrees=(-8, 8),\n",
        "                translate=(1/24, 1/24),\n",
        "                scale=(8/9, 10/9)\n",
        "            ),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.071371482,), std=(0.20764154,)),\n",
        "            Cutout(8, 12)\n",
        "        ])\n",
        "        self.normalize = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.071371482,), std=(0.20764154,))\n",
        "        ])\n",
        "        \n",
        "        # initialize labels and counts for class balancing\n",
        "        self.labels = torch.tensor(labels.to_numpy())\n",
        "        counts = labels.apply(pd.Series.value_counts).to_numpy().T\n",
        "        max_counts = np.nanmax(counts, axis=1, keepdims=True)\n",
        "        self.mod_counts = torch.tensor(max_counts % counts)\n",
        "        self.ratio_counts = torch.tensor(max_counts // counts)\n",
        "        self.current_counts = torch.zeros_like(self.mod_counts)\n",
        "        \n",
        "        self.augment = augment\n",
        "        self.balance = balance\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reset number of retrieved items of each class in current epoch.\"\"\"\n",
        "        self.current_counts = torch.zeros_like(self.mod_counts)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def _num_augmentations(self, labels):\n",
        "        \"\"\"Computes number of augmentations for given image labels.\n",
        "        \n",
        "        Args:\n",
        "            labels = [torch.Tensor] image labels of shape (3,)\n",
        "            \n",
        "        Returns [torch.Tensor]:\n",
        "            If self.balance is False, a tensor filled with ones is returned.\n",
        "            Otherwise, the number of augmentations will ensure that all the\n",
        "            classes are seen the same number of times for each subproblem.\n",
        "        \"\"\"\n",
        "        if not self.balance:  # one augmentation\n",
        "            return torch.tensor([1]*len(labels))\n",
        "        \n",
        "        # select current and modular counts for given labels\n",
        "        current_counts = self.current_counts[[0, 1, 2], labels]\n",
        "        self.current_counts[[0, 1, 2], labels] += 1\n",
        "        mod_counts = self.mod_counts[[0, 1, 2], labels]\n",
        "\n",
        "        # determine number of augmentations with possible extra augmentation\n",
        "        extra_augment = current_counts < mod_counts\n",
        "        num_augments = self.ratio_counts[[0, 1, 2], labels] + extra_augment\n",
        "\n",
        "        return num_augments.long()\n",
        "\n",
        "    def _augment_or_normalize(self, image):\n",
        "        \"\"\"Augments (including normalization) or normalizes image.\n",
        "        \n",
        "        Args:\n",
        "            image = [ndarray] Numpy array image of shape (SIZE, SIZE)\n",
        "            \n",
        "        Returns [torch.Tensor]\n",
        "            Augmented or normalized image with shape (1, 1, SIZE, SIZE).\n",
        "        \"\"\"\n",
        "        if self.augment:  # random affine, normalize, cutout\n",
        "            image = self.transform(image)\n",
        "        else:  # normalize\n",
        "            image = self.normalize(image)\n",
        "\n",
        "        return image.unsqueeze(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get images, labels, and number of augmentations.\n",
        "        \n",
        "        Args:\n",
        "            idx = [int] index of original image and labels\n",
        "            \n",
        "        Returns [torch.Tensor]*5:\n",
        "            images       = images tensor of shape (N, 1, SIZE, SIZE)\n",
        "            labels_graph = labels tensor of grapheme_root subproblem\n",
        "            labels_vowel = labels tensor of vowel_diacritic subproblem\n",
        "            labels_conso = labels tensor of consonant_diacritic subproblem\n",
        "            num_augments = number of augmentations of shape (1, 3)\n",
        "        \"\"\"\n",
        "        # select image and labels\n",
        "        image = self.images[idx]\n",
        "        labels = self.labels[idx]\n",
        "        \n",
        "        # determine number of augmentations per subproblem\n",
        "        num_augments = self._num_augmentations(labels)\n",
        "        \n",
        "        # transform or normalize image\n",
        "        images = self._augment_or_normalize(image)\n",
        "        for _ in range(max(num_augments) - 1):\n",
        "            images = torch.cat((images, self._augment_or_normalize(image)))\n",
        "\n",
        "        # repeat labels given number of augmentations\n",
        "        labels = [labels[i].repeat(num_augments[i]) for i in range(len(labels))]\n",
        "\n",
        "        # return images, labels, and number of augmentations as a 5-tuple\n",
        "        return (images,) + tuple(labels) + (num_augments.unsqueeze(0),)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y4Yk4QRlkAG7",
        "colab": {}
      },
      "source": [
        "def eval_metric(pred_dict, true_dict):\n",
        "    \"\"\"\n",
        "    Competition evaluation metric adapted from:\n",
        "    https://www.kaggle.com/c/bengaliai-cv19/overview/evaluation\n",
        "    The metric describes the weighted average of component \n",
        "    macro-averaged recalls.\n",
        "    \n",
        "    Args:\n",
        "        pred_dict = [dict] dictionary with components as keys and\n",
        "                           lists of predictions as values\n",
        "        true_dict = [dict] dictionary with components as key and\n",
        "                           lists of targets as values\n",
        "    \n",
        "    Returns [float]*4:\n",
        "        grapheme  = grapheme_root component macro-average recall\n",
        "        vowel     = vowel_diacritic component macro-average recall\n",
        "        consonant = consonant_diacritic component macro-average recall\n",
        "        total     = weighted average of component macro-averaged recalls\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    for key in ['grapheme', 'vowel', 'consonant']:\n",
        "        score = recall_score(true_dict[key], pred_dict[key], average='macro')\n",
        "        scores.append(score)\n",
        "        \n",
        "    scores.append(np.average(scores, weights=[2, 1, 1]))\n",
        "    return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3lZbC2IckAHA",
        "colab": {}
      },
      "source": [
        "def update_dicts(pred_dict, true_dict, preds, targets):\n",
        "    \"\"\"Updates two dictionaries given batches of values.\n",
        "    \n",
        "    Args:\n",
        "        pred_dict = [dict] dictionary with components as keys and\n",
        "                           lists of predictions as values\n",
        "        true_dict = [dict] dictionary with components as key and\n",
        "                           lists of targets as values\n",
        "        preds     = [tuple] sequence of tensors of (raw) predictions\n",
        "        targets   = [tuple] sequence of tensors of targets\n",
        "    \"\"\"\n",
        "    for key, y, t in zip(['grapheme', 'vowel', 'consonant'], preds, targets):\n",
        "        _, pred = torch.max(y.data, 1)\n",
        "        pred_list = pred.tolist()\n",
        "        target_list = t.tolist()\n",
        "        pred_dict[key] += pred_list\n",
        "        true_dict[key] += target_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pEzITCBkAHI"
      },
      "source": [
        "Naming convention:\n",
        "- `x` = input\n",
        "- `t` = target\n",
        "- `y` = predicted output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LbjosgSlAvuK",
        "colab": {}
      },
      "source": [
        "class CrossEntropySumLoss(nn.Module):\n",
        "    \"\"\"Neural network module to compute sum of cross entropy losses.\n",
        "\n",
        "    Attributes:\n",
        "        device = [torch.device] device to compute the loss on\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device):\n",
        "        \"\"\"Initializes the loss module\n",
        "\n",
        "        Args:\n",
        "            device = [torch.device] device to compute the loss on\n",
        "        \"\"\"\n",
        "        super(CrossEntropySumLoss, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"Sums cross entropy losses of given predictions and targets.\n",
        "        \n",
        "        Args:\n",
        "            input  = [tuple] sequence of tensors of (raw) predictions\n",
        "            target = [tuple] sequence of tensors of targets\n",
        "        \n",
        "        Returns [torch.Tensor]:\n",
        "            The grapheme_root, vowel_dacritic, consonant_diacritic,\n",
        "            and combined losses given the predictions and targets.\n",
        "        \"\"\"\n",
        "        losses = torch.zeros(1, dtype=torch.float32, device=self.device)\n",
        "        for y, t in zip(input, target):\n",
        "            t = t.to(self.device)\n",
        "            loss = F.cross_entropy(y, t).view(1)\n",
        "            losses = torch.cat((loss, losses))\n",
        "\n",
        "        losses[-1] = sum(losses[:-1])\n",
        "        return losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "--IKV3-5lX-s",
        "colab": {}
      },
      "source": [
        "num_iterations = 0\n",
        "num_batches = 0\n",
        "running_losses = torch.zeros(4)\n",
        "pred_dict = {'grapheme': [], 'vowel': [], 'consonant': []}\n",
        "true_dict = {'grapheme': [], 'vowel': [], 'consonant': []}\n",
        "\n",
        "\n",
        "def show_metrics(writer, losses=None, preds=None, targets=None,\n",
        "                 inc=True, eval_freq=100, end=False):\n",
        "    \"\"\"Show the losses and scores on TensorBoard.\n",
        "    \n",
        "    Args:\n",
        "        writer    = [SummaryWriter] TensorBoard writer of metrics\n",
        "        losses    = [torch.Tensor] subproblem losses and combined loss\n",
        "        preds     = [tuple] sequence of tensors of (raw) predictions\n",
        "        targets   = [tuple] sequence of tensors of targets\n",
        "        inc       = [bool] whether to increment the number of iterations\n",
        "        eval_freq = [int] number of iterations before the next TensorBoard\n",
        "                          update; if set to -1, TensorBoard never updates\n",
        "        end       = [bool] always shows metrics after epoch has ended\n",
        "    \"\"\"\n",
        "    global num_iterations\n",
        "    global num_batches\n",
        "    global running_losses\n",
        "    global pred_dict\n",
        "    global true_dict   \n",
        "    \n",
        "    if not end:\n",
        "        # increment total number of training iterations during run\n",
        "        num_iterations += inc\n",
        "\n",
        "        # increment number of batches during current epoch\n",
        "        num_batches += 1\n",
        "        \n",
        "        # accumulate metrics to smooth plots\n",
        "        running_losses += losses.data.cpu()\n",
        "        update_dicts(pred_dict, true_dict, preds, targets)\n",
        "    \n",
        "    # show metrics every eval_freq iterations or at the end of an epoch\n",
        "    if num_iterations % eval_freq == (eval_freq - 1) or end:\n",
        "        # show losses in TensorBoard\n",
        "        losses = running_losses / num_batches\n",
        "        writer.add_scalar('Loss/grapheme_root',\n",
        "                          losses[0], num_iterations)\n",
        "        writer.add_scalar('Loss/vowel_diacritic',\n",
        "                          losses[1], num_iterations)\n",
        "        writer.add_scalar('Loss/consonant_diacritic',\n",
        "                          losses[2], num_iterations)\n",
        "        writer.add_scalar('Loss/total',\n",
        "                          losses[3], num_iterations)\n",
        "\n",
        "        # show scores in TensorBoard\n",
        "        scores = eval_metric(pred_dict, true_dict)\n",
        "        writer.add_scalar('Score/grapheme_root',\n",
        "                          scores[0], num_iterations)\n",
        "        writer.add_scalar('Score/vowel_diacritic',\n",
        "                          scores[1], num_iterations)\n",
        "        writer.add_scalar('Score/consonant_diacritic',\n",
        "                          scores[2], num_iterations)\n",
        "        writer.add_scalar('Score/total',\n",
        "                          scores[3], num_iterations)\n",
        "        \n",
        "        # reset running variables\n",
        "        num_batches = 0\n",
        "        running_losses = torch.zeros(4)\n",
        "        pred_dict = {'grapheme': [], 'vowel': [], 'consonant': []}\n",
        "        true_dict = {'grapheme': [], 'vowel': [], 'consonant': []}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YKueMB5RlX-o",
        "colab": {}
      },
      "source": [
        "def validation(model, val_loader, val_writer, criterion):\n",
        "    \"\"\"Computes loss and score of current state of model on validation dataset.\n",
        "    \n",
        "    Args:\n",
        "        model      = [nn.Module] model to test with validation dataset\n",
        "        val_loader = [DataLoader] validation data loader\n",
        "        val_writer = [SummaryWriter] TensorBoard writer of validation metrics\n",
        "        criterion  = [nn.Module] neural network module to compute loss\n",
        "    \"\"\"\n",
        "    # set model mode to evaluation\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            x, t_graph, t_vowel, t_conso, _ = data\n",
        "            \n",
        "            # predict\n",
        "            y = model(x)\n",
        "            \n",
        "            # loss\n",
        "            t = t_graph, t_vowel, t_conso\n",
        "            losses = criterion(y, t)\n",
        "            \n",
        "            # accumulate but do not show validation metrics\n",
        "            show_metrics(val_writer, losses, y, t, inc=False, eval_freq=-1)\n",
        "            \n",
        "    # show validation metrics on TensorBoard\n",
        "    show_metrics(val_writer, end=True)\n",
        "    \n",
        "    # set model mode back to training\n",
        "    model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qW6ECh0vkAHL",
        "colab": {}
      },
      "source": [
        "def train(model, train_dataset, train_loader, train_writer,\n",
        "          val_loader, val_writer, optimizer, criterion, num_epochs=10):\n",
        "    \"\"\"Trains the model given train data and validates it given validation data.\n",
        "    \n",
        "    Args:\n",
        "        model         = [nn.Module] model to train and validate\n",
        "        train_dataset = [Dataset] train dataset\n",
        "        train_loader  = [DataLoader] train data loader\n",
        "        train_writer  = [SummaryWriter] TensorBoard writer of train metrics\n",
        "        val_loader    = [DataLoader] validation data loader\n",
        "        val_writer    = [SummaryWriter] TensorBoard writer of validation metrics\n",
        "        optimizer     = [Optimizer] optimizer to update the model\n",
        "        criterion     = [nn.Module] neural network module to compute loss\n",
        "        num_epochs    = [int] number of iterations of the train dataset\n",
        "    \"\"\"\n",
        "    for epoch in range(num_epochs):\n",
        "        for data in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
        "            x, t_graph, t_vowel, t_conso, num_augments = data # depends on what is returned by Dataset.__getitem__\n",
        "                       \n",
        "            # predict\n",
        "            y = model(x, num_augments)\n",
        "\n",
        "            # loss\n",
        "            t = t_graph, t_vowel, t_conso\n",
        "            losses = criterion(y, t)\n",
        "\n",
        "            # update\n",
        "            optimizer.zero_grad() \n",
        "            losses[-1].backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # show train metrics every 100 iterations in TensorBoard\n",
        "            show_metrics(train_writer, losses, y, t)\n",
        "\n",
        "        # show train metrics at end of epoch\n",
        "        show_metrics(train_writer, end=True)\n",
        "                \n",
        "        # evaluate model on validation data\n",
        "        validation(model, val_loader, val_writer, criterion)\n",
        "        \n",
        "        # reset dataset to keep class balance\n",
        "        train_dataset.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZMlCPo-nlX-0",
        "outputId": "bd5096d6-5068-4690-e22e-0bfcc20bd191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "SIZE = 128\n",
        "BATCH_SIZE = 128\n",
        "PATH = 'model.pt'\n",
        "\n",
        "# use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "# initialize network and show summary\n",
        "model = ZeroNet(device).train()\n",
        "summary(model, input_size=(1, SIZE, SIZE), device=str(device))\n",
        "\n",
        "# initialize optimizer and criterion\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = CrossEntropySumLoss(device)\n",
        "\n",
        "# training set\n",
        "linux = sys.platform.startswith('linux')\n",
        "train_dataset = BengaliDataset(train_images, train_labels,\n",
        "                               augment=False, balance=False)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=4*linux, collate_fn=_new_default_collate)\n",
        "\n",
        "# validation set\n",
        "val_dataset = BengaliDataset(val_images, val_labels,\n",
        "                             augment=False, balance=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=4*linux, collate_fn=_new_default_collate)\n",
        "\n",
        "# TensorBoard writers\n",
        "current_time = datetime.now().strftime(\"%Y-%m-%d/%H'%M'%S\")\n",
        "train_writer = SummaryWriter(f'runs/{current_time}/train')\n",
        "train_writer.add_graph(model, iter(train_loader).next()[0])\n",
        "val_writer = SummaryWriter(f'runs/{current_time}/validation')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 10, 126, 126]             100\n",
            "            Conv2d-2           [-1, 20, 40, 40]           1,820\n",
            "         Dropout2d-3           [-1, 20, 40, 40]               0\n",
            "            Linear-4                  [-1, 256]         865,536\n",
            "            Linear-5                  [-1, 168]          43,176\n",
            "            Linear-6                   [-1, 11]           2,827\n",
            "            Linear-7                    [-1, 7]           1,799\n",
            "================================================================\n",
            "Total params: 915,258\n",
            "Trainable params: 915,258\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 1.70\n",
            "Params size (MB): 3.49\n",
            "Estimated Total Size (MB): 5.26\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:75: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nei3Dc8YkAHR",
        "outputId": "a2549e3c-5456-46e2-889c-f20c5e80a4c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "42ccdb5125fd4cc789827c3aa6b25b4a",
            "65c30daa4181472298f6c158db902500",
            "ce81f3f41ae449289fec4f9dd4b5c3b6",
            "28ea77d195db42dbac6747ef56a6951a",
            "7f2c9a48e8714157aac1cfce45d4350b",
            "3915de1396e343eebecb8b5e949787e9",
            "f8eb4a08163045339793547bb8803c64",
            "c7048b6a73bb4a6e829f146eef1fda55"
          ]
        }
      },
      "source": [
        "train(model, train_dataset, train_loader, train_writer,\n",
        "      val_loader, val_writer, optimizer, criterion, num_epochs=50)\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42ccdb5125fd4cc789827c3aa6b25b4a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch 1/50', max=1256, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-04a6b944014c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train(model, train_dataset, train_loader, train_writer,\n\u001b[0;32m----> 2\u001b[0;31m       val_loader, val_writer, optimizer, criterion, num_epochs=50)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-17e4473642d6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, train_loader, train_writer, val_loader, val_writer, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_augments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-220460fdd5cd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, num_augments)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# x = self.fc2(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mx_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_vowel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_conso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_augments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0my_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0my_vowel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vowel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-220460fdd5cd>\u001b[0m in \u001b[0;36m_split_vectors\u001b[0;34m(vectors, num_augments)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# determine the indices of the latent vectors for each subproblem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mvowel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mconso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-220460fdd5cd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# determine the indices of the latent vectors for each subproblem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mvowel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mconso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    460\u001b[0m                           \u001b[0;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                           'incorrect results).', category=RuntimeWarning)\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}